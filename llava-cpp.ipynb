{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d56d5c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to load shared library 'c:\\Users\\basal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\lib\\llama.dll': Could not find module 'c:\\Users\\basal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\lib\\llama.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\basal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\_ctypes_extensions.py:67\u001b[39m, in \u001b[36mload_shared_library\u001b[39m\u001b[34m(lib_base_name, base_path)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcdll_args\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\basal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ctypes\\__init__.py:379\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Could not find module 'c:\\Users\\basal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\lib\\llama.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Llama\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbase64\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# --- 1. Конфигурация ---\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Убедитесь, что этот путь правильный!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\basal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama_cpp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mllama\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      4\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.3.16\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\basal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\llama_cpp.py:38\u001b[39m\n\u001b[32m     36\u001b[39m _base_path = pathlib.Path(os.path.abspath(os.path.dirname(\u001b[34m__file__\u001b[39m))) / \u001b[33m\"\u001b[39m\u001b[33mlib\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _override_base_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m pathlib.Path(_override_base_path)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Load the library\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m _lib = \u001b[43mload_shared_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_lib_base_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_base_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m ctypes_function = ctypes_function_for_shared_library(_lib)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# from ggml.h\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# // NOTE: always add types at the end of the enum to keep backward compatibility\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# enum ggml_type {\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m \u001b[38;5;66;03m#     GGML_TYPE_COUNT,\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# };\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\basal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\_ctypes_extensions.py:69\u001b[39m, in \u001b[36mload_shared_library\u001b[39m\u001b[34m(lib_base_name, base_path)\u001b[39m\n\u001b[32m     67\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m ctypes.CDLL(\u001b[38;5;28mstr\u001b[39m(lib_path), **cdll_args)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     68\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to load shared library \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlib_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     72\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mShared library with base name \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlib_base_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m )\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to load shared library 'c:\\Users\\basal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\lib\\llama.dll': Could not find module 'c:\\Users\\basal\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\llama_cpp\\lib\\llama.dll' (or one of its dependencies). Try using the full path with constructor syntax."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from llama_cpp import Llama\n",
    "import base64\n",
    "\n",
    "# --- 1. Конфигурация ---\n",
    "# Убедитесь, что этот путь правильный!\n",
    "MODEL_PATH = \"./models/llava-v1.6-vicuna-13b.Q4_K_M.gguf\" \n",
    "IMG_DIR = \"img\"\n",
    "RESULTS_FILE = \"llava_local_13b_results.csv\"\n",
    "PROMPT = \"Скажи, изображена ли на данном фото кошка породы \\\"Британская короткошерстная\\\", дай коротки ответ \\\"да\\\" или \\\"нет\\\". Опиши одним словом каждый признак, который убедил тебя в правильности или неправильности ответа. Если это не британская короткошерстная кошка, скажи какая это порода.\"\n",
    "\n",
    "# --- Настройка GPU Offloading ---\n",
    "# Это самый важный параметр для вашей RTX 4060 (8 ГБ VRAM).\n",
    "# Он определяет, сколько слоев модели выгрузить в видеопамять.\n",
    "# 13B-модель (~8 ГБ) не поместится целиком.\n",
    "# Хорошее начальное значение - 35. Если будут ошибки \"out of memory\", уменьшите до 30.\n",
    "# Если VRAM будет использоваться не полностью, можно увеличить до 38.\n",
    "N_GPU_LAYERS = 35\n",
    "\n",
    "# --- 2. Загрузка модели ---\n",
    "llm = None\n",
    "# Проверяем, существует ли файл модели, прежде чем пытаться его загрузить\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"Загрузка модели... Это может занять несколько минут, пока слои выгружаются на GPU.\")\n",
    "    try:\n",
    "        llm = Llama(\n",
    "            model_path=MODEL_PATH,\n",
    "            n_ctx=2048,  # Размер контекста (окна внимания)\n",
    "            n_gpu_layers=N_GPU_LAYERS, # <-- Применяем нашу настройку\n",
    "            verbose=False # Отключаем лишний технический вывод\n",
    "        )\n",
    "        print(\"✅ Модель успешно загружена!\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Ошибка загрузки модели: {e}\")\n",
    "else:\n",
    "    print(f\"❌ Файл модели не найден по пути: {MODEL_PATH}\")\n",
    "    print(\"Пожалуйста, убедитесь, что вы скачали модель и поместили ее в папку /models.\")\n",
    "\n",
    "\n",
    "# --- Вспомогательная функция для кодирования изображения ---\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# --- 3. Основной цикл обработки ---\n",
    "if llm:\n",
    "    try:\n",
    "        image_files = sorted([f for f in os.listdir(IMG_DIR) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        results_data = []\n",
    "        \n",
    "        print(f\"Начинаем обработку {len(image_files)} изображений локально на вашем GPU...\")\n",
    "        for filename in tqdm(image_files, desc=\"LLaVA Local 13B\"):\n",
    "            image_path = os.path.join(IMG_DIR, filename)\n",
    "            \n",
    "            try:\n",
    "                base64_image = encode_image(image_path)\n",
    "                \n",
    "                # Создаем запрос в формате, который понимает LLaVA через llama-cpp-python\n",
    "                response = llm.create_chat_completion(\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "                                {\"type\": \"text\", \"text\": PROMPT}\n",
    "                            ]\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "                result_text = response['choices'][0]['message']['content'].strip()\n",
    "                results_data.append({\"filename\": filename, \"llava_13b_response\": result_text})\n",
    "\n",
    "            except Exception as e:\n",
    "                # Если одна картинка вызовет ошибку, мы запишем ее и продолжим\n",
    "                results_data.append({\"filename\": filename, \"llava_13b_response\": f\"Error: {e}\"})\n",
    "        \n",
    "        df_results = pd.DataFrame(results_data)\n",
    "        df_results.to_csv(RESULTS_FILE, index=False, encoding='utf-8')\n",
    "        print(f\"\\n✅ Результаты сохранены в '{RESULTS_FILE}'.\")\n",
    "        display(df_results.head())\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Ошибка: Директория '{IMG_DIR}' не найдена.\")\n",
    "else:\n",
    "    print(\"\\n❌ Основной процесс не был запущен, так как модель не была загружена.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
